# Kasparro Backend - Cryptocurrency ETL System

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-009688.svg)](https://fastapi.tiangolo.com)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-316192.svg)](https://www.postgresql.org)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://www.docker.com)
[![Tests](https://img.shields.io/badge/Tests-34%2F34%20Passing-success.svg)](.)

A production-ready ETL pipeline that ingests cryptocurrency data from multiple sources, transforms it into a unified schema, and exposes it through a RESTful API.

---

## Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Quick Start](#-quick-start)
- [API Documentation](#-api-documentation)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Testing](#-testing)
- [Configuration](#-configuration)
- [Deployment](#-deployment)
- [Troubleshooting](#-troubleshooting)

---

## Overview

This system implements a complete ETL (Extract, Transform, Load) pipeline for cryptocurrency market data:

- **Extracts** data from 3 sources: CoinPaprika API, CoinGecko API, and CSV files
- **Transforms** data using Pydantic validation into a unified schema
- **Loads** cleaned data into PostgreSQL with raw data preservation
- **Exposes** data through FastAPI REST endpoints with filtering and pagination
- **Tracks** ETL runs and maintains checkpoints for incremental ingestion

**Built with:**
- [FastAPI](https://fastapi.tiangolo.com) - Modern Python web framework
- [PostgreSQL](https://www.postgresql.org) - Reliable relational database
- [SQLAlchemy](https://www.sqlalchemy.org) - Python ORM
- [Pydantic](https://pydantic-docs.helpmanual.io) - Data validation
- [Docker](https://www.docker.com) - Containerization

---

## Features

### ETL Pipeline
âœ… Multi-source data extraction (CoinPaprika, CoinGecko, CSV)  
âœ… Schema normalization with Pydantic validation  
âœ… Incremental ingestion using checkpoint system  
âœ… Raw data preservation for reprocessing  
âœ… Comprehensive error handling and logging  
âœ… ETL run tracking and metadata  

### REST API
âœ… Health check with database status  
âœ… Data retrieval with filtering by source/crypto  
âœ… Pagination and sorting support  
âœ… ETL statistics and run history  
âœ… Interactive API documentation (Swagger UI)  
âœ… Request tracking with unique IDs  

### Infrastructure
âœ… Fully Dockerized with docker-compose  
âœ… PostgreSQL with health checks  
âœ… Test suite with 34 passing tests (100%)  
âœ… Isolated test database  

---

## Quick Start

### Prerequisites

Before you begin, ensure you have:

- **Docker Desktop** - [Download here](https://www.docker.com/products/docker-desktop)
- **Git** - [Download here](https://git-scm.com/downloads)
- **Python 3.11+** (for local development) - [Download here](https://www.python.org/downloads/)

### Method 1: Docker (Recommended) ğŸ³

```bash
# Clone the repository
git clone https://github.com/yourusername/kasparro-backend-Krithika-M.git
cd kasparro-backend-Krithika-M

# Start all services (PostgreSQL + API)
docker-compose up -d

# Verify containers are running
docker-compose ps

# View logs
docker-compose logs -f api
```

**Access the application:**
- API: http://localhost:8000
- API Docs: http://localhost:8000/docs
- Health Check: http://localhost:8000/health

### Method 2: Local Development 

```bash
# Create and activate virtual environment
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
echo "DATABASE_URL=postgresql://kasparro:password@127.0.0.1:5433/kasparro" > .env
echo "CSV_FILE_PATH=data/crypto_sample.csv" >> .env
echo "BATCH_SIZE=100" >> .env

# Start PostgreSQL in Docker
docker-compose up -d postgres

# Initialize database schema
python services/database.py

# Run ETL pipeline
python -m ingestion.etl

# Start API server
uvicorn api.main:app --reload --port 8000
```

---

## API Documentation

### Base URL

```
http://localhost:8000
```

### Endpoints

#### Root Endpoint

```http
GET /
```

**Response:**
```json
{
  "message": "Welcome to Kasparro Backend API",
  "version": "1.0.0",
  "docs": "/docs",
  "endpoints": {
    "health": "/health",
    "data": "/data",
    "stats": "/stats"
  }
}
```

---

####  Health Check

```http
GET /health
```

Check system health and database connectivity.

**Response:**
```json
{
  "status": "healthy",
  "database": "connected",
  "timestamp": "2025-12-27T18:30:00.123456Z",
  "last_etl_run": {
    "run_id": 5,
    "source": "coinpaprika,coingecko,csv",
    "records_processed": 25,
    "success": true,
    "started_at": "2025-12-27T18:29:50Z",
    "ended_at": "2025-12-27T18:29:58Z"
  }
}
```

---

####  Get Cryptocurrency Data

```http
GET /data
```

Retrieve cryptocurrency data with filtering, pagination, and sorting.

**Query Parameters:**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `source` | string | all | Filter by source: `coinpaprika`, `coingecko`, `csv` |
| `crypto_id` | string | all | Filter by crypto ID (e.g., `btc-bitcoin`) |
| `limit` | integer | 10 | Records per page (max: 100) |
| `offset` | integer | 0 | Pagination offset |
| `sort_by` | string | `created_at` | Sort field: `created_at`, `price_usd`, `market_cap_usd` |
| `order` | string | `desc` | Sort order: `asc`, `desc` |

**Example Request:**
```bash
curl -X GET "http://localhost:8000/data?source=coinpaprika&limit=5&sort_by=price_usd&order=desc"
```

**Response:**
```json
{
  "total": 10,
  "page": 1,
  "page_size": 5,
  "data": [
    {
      "id": 1,
      "crypto_id": "btc-bitcoin",
      "crypto_name": "Bitcoin",
      "price_usd": 87515.94,
      "market_cap_usd": 1747511549156.0,
      "volume_24h_usd": 16497951286.61,
      "change_24h_percent": 0.26,
      "data_source": "coinpaprika",
      "created_at": "2025-12-27T16:36:54.422714"
    }
  ]
}
```

---

####  Get Specific Cryptocurrency

```http
GET /data/{crypto_id}
```

Get the latest data for a specific cryptocurrency.

**Example Request:**
```bash
curl -X GET "http://localhost:8000/data/btc-bitcoin"
```

**Response:**
```json
{
  "id": 1,
  "crypto_id": "btc-bitcoin",
  "crypto_name": "Bitcoin",
  "price_usd": 87515.94,
  "market_cap_usd": 1747511549156.0,
  "volume_24h_usd": 16497951286.61,
  "change_24h_percent": 0.26,
  "data_source": "coinpaprika",
  "created_at": "2025-12-27T16:36:54.422714"
}
```

---

####  ETL Statistics

```http
GET /stats
```

Get detailed ETL statistics including run history and source breakdown.

**Response:**
```json
{
  "total_cleaned_records": 25,
  "total_raw_api_records": 20,
  "total_raw_csv_records": 5,
  "sources": [
    {
      "source": "coinpaprika",
      "total_records": 10,
      "last_checkpoint": 10,
      "last_update": "2025-12-27T16:30:10Z"
    }
  ],
  "recent_etl_runs": [
    {
      "run_id": 2,
      "source": "coinpaprika,coingecko,csv",
      "started_at": "2025-12-27T16:30:08Z",
      "ended_at": "2025-12-27T16:30:10Z",
      "records_processed": 25,
      "success": true,
      "duration_seconds": 2.45
    }
  ]
}
```

---

####  Statistics Summary

```http
GET /stats/summary
```

Quick summary of total records and last run.

**Response:**
```json
{
  "total_records": 25,
  "last_successful_run": "2025-12-27T16:36:56.740417",
  "records_by_source": {
    "csv": 5,
    "coingecko": 10,
    "coinpaprika": 10
  }
}
```

---

### Interactive API Documentation

Visit http://localhost:8000/docs for Swagger UI with interactive testing.

---

##  Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA SOURCES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CoinPaprika API â”‚  CoinGecko API   â”‚    CSV Files         â”‚
â”‚    (Free Tier)   â”‚   (Public API)   â”‚  crypto_sample.csv   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  INGESTION LAYER     â”‚
                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  â”‚  API Clients   â”‚  â”‚
                 â”‚  â”‚  - CoinPaprika â”‚  â”‚
                 â”‚  â”‚  - CoinGecko   â”‚  â”‚
                 â”‚  â”‚  - CSV Reader  â”‚  â”‚
                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  â”‚  Transformers  â”‚  â”‚
                 â”‚  â”‚  - Normalize   â”‚  â”‚
                 â”‚  â”‚  - Validate    â”‚  â”‚
                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   ETL PIPELINE       â”‚
                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  â”‚  1. Extract    â”‚  â”‚
                 â”‚  â”‚  2. Transform  â”‚  â”‚
                 â”‚  â”‚  3. Load       â”‚  â”‚
                 â”‚  â”‚  4. Checkpoint â”‚  â”‚
                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   POSTGRESQL DB      â”‚
                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  â”‚ raw_api_data   â”‚  â”‚
                 â”‚  â”‚ raw_csv_data   â”‚  â”‚
                 â”‚  â”‚ cleaned_data   â”‚  â”‚
                 â”‚  â”‚ etl_runs       â”‚  â”‚
                 â”‚  â”‚ etl_checkpointsâ”‚  â”‚
                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     FASTAPI          â”‚
                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  â”‚ GET /health    â”‚  â”‚
                 â”‚  â”‚ GET /data      â”‚  â”‚
                 â”‚  â”‚ GET /stats     â”‚  â”‚
                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                       API CLIENTS
```

### Component Breakdown

**1. Ingestion Layer** (`ingestion/`)
- `clients.py` - HTTP clients for external APIs
- `transformers.py` - Data transformation logic
- `schemas.py` - Pydantic validation models
- `etl.py` - Main ETL orchestrator

**2. API Layer** (`api/`)
- `main.py` - FastAPI application
- `routes/data.py` - Data endpoints
- `routes/health.py` - Health checks
- `routes/stats.py` - Statistics endpoints

**3. Database Layer** (`services/`)
- `database.py` - SQLAlchemy models and connection

**4. Testing** (`tests/`)
- Unit tests for all components
- Integration tests for ETL pipeline
- API endpoint tests
- Failure scenario tests

---

##  Project Structure

```
kasparro-backend-Krithika-M/
â”‚
â”œâ”€â”€ api/                              # FastAPI application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                       # Main API entry point
â”‚   â””â”€â”€ routes/                       # API route handlers
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ data.py                   # /data endpoints
â”‚       â”œâ”€â”€ health.py                 # /health endpoint
â”‚       â””â”€â”€ stats.py                  # /stats endpoints
â”‚
â”œâ”€â”€ ingestion/                        # ETL pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl.py                        # Main ETL orchestrator
â”‚   â”œâ”€â”€ clients.py                    # API clients (CoinPaprika, CoinGecko)
â”‚   â”œâ”€â”€ transformers.py               # Data transformation logic
â”‚   â””â”€â”€ schemas.py                    # Pydantic validation schemas
â”‚
â”œâ”€â”€ services/                         # Core services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ database.py                   # Database models & connection
â”‚
â”œâ”€â”€ tests/                            # Test suite (34 tests)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                   # Pytest fixtures
â”‚   â”œâ”€â”€ test_api.py                   # API endpoint tests
â”‚   â”œâ”€â”€ test_database.py              # Database operation tests
â”‚   â”œâ”€â”€ test_etl.py                   # ETL pipeline tests
â”‚   â”œâ”€â”€ test_transformers.py          # Transformer tests
â”‚   â””â”€â”€ test_failure_scenarios.py     # Error handling tests
â”‚
â”œâ”€â”€ data/                             # Data files
â”‚   â””â”€â”€ crypto_sample.csv             # Sample cryptocurrency data
â”‚
â”œâ”€â”€ logs/                             # Application logs
â”‚
â”œâ”€â”€ .env                              # Environment variables
â”œâ”€â”€ .gitignore                        # Git ignore rules
â”œâ”€â”€ docker-compose.yml                # Docker orchestration
â”œâ”€â”€ Dockerfile                        # API container definition
â”œâ”€â”€ Makefile                          # Build automation
â”œâ”€â”€ pytest.ini                        # Pytest configuration
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                         # This file
```

---

##  Testing

### Run All Tests

```bash
# Run all 34 tests
pytest tests/ -v

# Run with coverage report
pytest tests/ --cov=. --cov-report=term-missing --cov-report=html

# Open coverage report
open htmlcov/index.html  # Mac/Linux
start htmlcov/index.html  # Windows
```

### Run Specific Test Categories

```bash
# API tests only
pytest tests/test_api.py -v

# ETL tests only
pytest tests/test_etl.py -v

# Database tests only
pytest tests/test_database.py -v

# Transformer tests only
pytest tests/test_transformers.py -v
```

### Test Coverage

âœ… **34/34 tests passing (100%)**

**Coverage includes:**
- âœ… API endpoint functionality
- âœ… Database CRUD operations
- âœ… ETL pipeline execution
- âœ… Data transformation logic
- âœ… Checkpoint system
- âœ… Error handling and recovery
- âœ… Data validation

---

##  Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# Database Configuration
DATABASE_URL=postgresql://kasparro:password@127.0.0.1:5433/kasparro

# CSV File Path
CSV_FILE_PATH=data/crypto_sample.csv

# ETL Configuration
BATCH_SIZE=100
```

### Docker Configuration

The `docker-compose.yml` defines two services:

```yaml
services:
  postgres:
    image: postgres:15
    container_name: kasparro_db
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: kasparro
      POSTGRES_PASSWORD: password
      POSTGRES_DB: kasparro
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kasparro"]
      interval: 10s
      timeout: 5s
      retries: 5

  api:
    build: .
    container_name: kasparro_api
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://kasparro:password@postgres:5432/kasparro
```

---

##  Deployment

### Docker Deployment

```bash
# Build and start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Rebuild after code changes
docker-compose up -d --build
```


---

## ğŸŒ Live Production Deployment

**Deployed Application:** [https://kasparro-backend-krithika-m.onrender.com](https://kasparro-backend-krithika-m.onrender.com)

### Live Endpoints

- **API Documentation:** [https://kasparro-backend-krithika-m.onrender.com/docs](https://kasparro-backend-krithika-m.onrender.com/docs)
- **Health Check:** [https://kasparro-backend-krithika-m.onrender.com/health](https://kasparro-backend-krithika-m.onrender.com/health)
- **Get Data:** [https://kasparro-backend-krithika-m.onrender.com/data](https://kasparro-backend-krithika-m.onrender.com/data)
- **Statistics:** [https://kasparro-backend-krithika-m.onrender.com/stats](https://kasparro-backend-krithika-m.onrender.com/stats)

### Quick Test

Check health
curl https://kasparro-backend-krithika-m.onrender.com/health

Get data
curl https://kasparro-backend-krithika-m.onrender.com/data?limit=5

View statistics
curl https://kasparro-backend-krithika-m.onrender.com/stats


### Deployment Details

- **Platform:** Render.com (Free Tier)
- **Database:** PostgreSQL 15 (Cloud)
- **Region:** Singapore (Asia-Pacific)
- **Container:** Docker
- **Auto Deploy:** Enabled (on every push to main branch)

### Automated ETL Scheduling

The ETL pipeline is scheduled to run automatically every hour using **GitHub Actions**.

**Cron Schedule:** `0 * * * *` (Every hour at minute 0)

**Workflow File:** `.github/workflows/etl-cron.yml`

The workflow:
1. Triggers every hour automatically
2. Checks the health endpoint to verify system status
3. Monitors ETL execution via the statistics endpoint
4. Logs are available in the GitHub Actions tab

**View Workflow Runs:** [GitHub Actions](https://github.com/Krithika-MH/kasparro-backend-Krithika-M/actions)

##  Troubleshooting

### Database Connection Issues

```bash
# Check if PostgreSQL container is running
docker-compose ps

# View database logs
docker-compose logs postgres

# Restart database
docker-compose restart postgres

# Connect to database directly
docker exec -it kasparro_db psql -U kasparro -d kasparro
```

### Port Already in Use

```bash
# Find process using port 8000 (Windows)
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Find process using port 8000 (Mac/Linux)
lsof -ti:8000 | xargs kill -9
```

### Test Database Issues

```bash
# Recreate test database
docker exec -it kasparro_db psql -U kasparro -d postgres -c "DROP DATABASE IF EXISTS kasparro_test;"
docker exec -it kasparro_db psql -U kasparro -d postgres -c "CREATE DATABASE kasparro_test;"

# Run tests again
pytest tests/ -v
```

### ETL Pipeline Issues

```bash
# Check ETL logs
docker-compose logs api | grep ETL

# Run ETL manually
docker exec -it kasparro_api python -m ingestion.etl

# Check checkpoint status
docker exec -it kasparro_db psql -U kasparro -d kasparro -c "SELECT * FROM etl_checkpoints;"
```

### API Not Responding

```bash
# Check API container status
docker-compose ps api

# View API logs
docker-compose logs -f api

# Restart API
docker-compose restart api

# Test health endpoint
curl http://localhost:8000/health
```

---

##  Performance Metrics

- **ETL Processing:** ~10 seconds for 25 records
- **API Response Time:** < 100ms average
- **Database Queries:** Optimized with indexes on `crypto_id` and `created_at`
- **Concurrent Requests:** Supports 100+ simultaneous API calls

---

##  Security

- âœ… Environment variables for sensitive data
- âœ… No hardcoded credentials
- âœ… SQL injection protection via SQLAlchemy ORM
- âœ… Input validation using Pydantic schemas
- âœ… CORS configuration ready for production

---

##  Dependencies

**Core:**
- FastAPI - Web framework
- SQLAlchemy - ORM
- Pydantic - Data validation
- PostgreSQL - Database
- Requests - HTTP client
- python-dotenv - Environment management

**Development:**
- Pytest - Testing framework
- Docker - Containerization
- Uvicorn - ASGI server

See `requirements.txt` for complete list with versions.

---

## Author

**M Krithika **  
---


